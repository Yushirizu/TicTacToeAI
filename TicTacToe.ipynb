{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOqdAxjxUmjQjoBsjwVCjr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import traceback\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "def print_exception():\n",
        "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
        "    traceback.print_exception(exc_type, exc_value, exc_traceback)\n",
        "\n",
        "# Define the game environment\n",
        "class TicTacToeEnv:\n",
        "    def __init__(self):\n",
        "        self.board = [[0, 0, 0] for _ in range(3)]\n",
        "        self.player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [[0, 0, 0] for _ in range(3)]\n",
        "        self.player = 1\n",
        "        return [0 for _ in range(9)]\n",
        "        \n",
        "    def step(self, action):\n",
        "        x, y = action\n",
        "        if self.board[x][y] == 0:\n",
        "            self.board[x][y] = self.player\n",
        "            self.player = -self.player\n",
        "\n",
        "            # Check if the game is over\n",
        "            if self.check_game_over():\n",
        "                return self.board, self.get_reward(), True\n",
        "            else:\n",
        "                return self.board, 0, False\n",
        "        else:\n",
        "            return self.board, -1, True\n",
        "\n",
        "    def check_game_over(self):\n",
        "        # Check rows\n",
        "        for row in self.board:\n",
        "            if row[0] == row[1] == row[2] and row[0] != 0:\n",
        "                return True\n",
        "\n",
        "        # Check columns\n",
        "        for col in range(3):\n",
        "            if self.board[0][col] == self.board[1][col] == self.board[2][col] and self.board[0][col] != 0:\n",
        "                return True\n",
        "\n",
        "        # Check diagonals\n",
        "        if self.board[0][0] == self.board[1][1] == self.board[2][2] and self.board[0][0] != 0:\n",
        "            return True\n",
        "        if self.board[0][2] == self.board[1][1] == self.board[2][0] and self.board[0][2] != 0:\n",
        "            return True\n",
        "\n",
        "        # Check if the board is full\n",
        "        for row in self.board:\n",
        "            for cell in row:\n",
        "                if cell == 0:\n",
        "                    return False\n",
        "\n",
        "        # If none of the above conditions are met, the game is a draw\n",
        "        return True\n",
        "\n",
        "    def get_reward(self):\n",
        "        # Check if player 1 won\n",
        "        if self.check_win(1):\n",
        "            return 1\n",
        "        # Check if player -1 won\n",
        "        elif self.check_win(-1):\n",
        "            return -1\n",
        "        # Otherwise, the game is a draw\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def check_win(self, player):\n",
        "        # Check rows\n",
        "        for row in self.board:\n",
        "            if row[0] == row[1] == row[2] == player:\n",
        "                return True\n",
        "\n",
        "        # Check columns\n",
        "        for col in range(3):\n",
        "            if self.board[0][col] == self.board[1][col] == self.board[2][col] == player:\n",
        "                return True\n",
        "\n",
        "        # Check diagonals\n",
        "        if self.board[0][0] == self.board[1][1] == self.board[2][2] == player:\n",
        "            return True\n",
        "        if self.board[0][2] == self.board[1][1] == self.board[2][0] == player:\n",
        "            return True\n",
        "\n",
        "        # If none of the above conditions are met, the player has not won\n",
        "        return False\n",
        "\n",
        "# Define the AI agent\n",
        "class TicTacToeAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TicTacToeAgent, self).__init__()\n",
        "        self.fc1 = nn.Linear(9, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.FloatTensor(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def act(self, state):\n",
        "        # Convert the state to a tensor\n",
        "        state = torch.FloatTensor(state)\n",
        "\n",
        "        # Use the forward pass to get the action scores\n",
        "        action_scores = self.forward(state)\n",
        "\n",
        "        # Choose the action with the highest score\n",
        "        _, action = torch.max(action_scores, dim=0)\n",
        "\n",
        "        # Convert the action to a tuple of coordinates\n",
        "        return (action // 3, action % 3)\n",
        "\n",
        "# Play a game of tic-tac-toe between two agents\n",
        "def play_game(agent_1, agent_2):\n",
        "    env = TicTacToeEnv()\n",
        "    state = env.reset()\n",
        "    player = 1\n",
        "\n",
        "    while True:\n",
        "        # Agent 1 plays\n",
        "        if player == 1:\n",
        "            action = agent_1.act(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            if done:\n",
        "                return reward\n",
        "\n",
        "        # Agent 2 plays\n",
        "        else:\n",
        "            action = agent_2.act(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            if done:\n",
        "                return reward\n",
        "\n",
        "        player = -player\n",
        "\n",
        "# Train an agent using deep reinforcement learning\n",
        "def train(agent, num_episodes, discount_factor=0.99):\n",
        "    optimizer = optim.Adam(agent.parameters())\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        # Play a game against a random agent\n",
        "        reward = play_game(agent, RandomAgent())\n",
        "        # Compute the loss\n",
        "        loss = -torch.log(agent.forward(state)[action]) * reward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print the loss every 1000 episodes\n",
        "        if i % 1000 == 0:\n",
        "            print(f'Episode {i}: loss = {loss.item():.4f}')\n",
        "\n",
        "# Define the random agent\n",
        "class RandomAgent:\n",
        "    def act(self, state):\n",
        "        return random.choice([(i, j) for i in range(3) for j in range(3) if state[3 * i + j] == 0])\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Create the agent\n",
        "    agent = TicTacToeAgent()\n",
        "\n",
        "    # Train the agent\n",
        "    train(agent, 10000)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "555o-dkC2_-H",
        "outputId": "adefc770-4c73-4a7a-cfcb-384f2c880397"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, num_episodes, discount_factor)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Play a game against a random agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(agent_1, agent_2)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Agent 2 plays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# Main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bf40fe0353fa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# Main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}