{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5NtFTk4Ws6fH8RG+bj+Ax"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "FEMfqqk-nyXl",
        "outputId": "eb922f35-e54a-4cf6-a60b-88693f0d68e7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bebe3e5fc228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Train the AI agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicTacToeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicTacToeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-bebe3e5fc228>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTicTacToeAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 't'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the game environment\n",
        "class TicTacToeEnv:\n",
        "  def __init__(self):\n",
        "    self.board = [[0, 0, 0] for _ in range(3)]\n",
        "    self.player = 1\n",
        "  \n",
        "  def reset(self):\n",
        "    self.board = [[0, 0, 0] for _ in range(3)]\n",
        "    self.player = 1\n",
        "    return [0 for _ in range(9)]\n",
        "  \n",
        "  def step(self, action):\n",
        "    x, y = action\n",
        "    if self.board[x][y] == 0:\n",
        "      self.board[x][y] = self.player\n",
        "      self.player = -self.player\n",
        "      \n",
        "      # Check if the game is over\n",
        "      if self.check_game_over():\n",
        "        return self.board, self.get_reward(), True\n",
        "      else:\n",
        "        return self.board, 0, False\n",
        "    else:\n",
        "      return self.board, -1, True\n",
        "  \n",
        "  def check_game_over(self):\n",
        "    # Check rows\n",
        "    for row in self.board:\n",
        "      if row[0] == row[1] == row[2] and row[0] != 0:\n",
        "        return True\n",
        "    \n",
        "    # Check columns\n",
        "    for col in range(3):\n",
        "      if self.board[0][col] == self.board[1][col] == self.board[2][col] and self.board[0][col] != 0:\n",
        "        return True\n",
        "    \n",
        "    # Check diagonals\n",
        "    if self.board[0][0] == self.board[1][1] == self.board[2][2] and self.board[0][0] != 0:\n",
        "      return True\n",
        "    if self.board[0][2] == self.board[1][1] == self.board[2][0] and self.board[0][2] != 0:\n",
        "      return True\n",
        "    \n",
        "    # Check if the board is full\n",
        "    for row in self.board:\n",
        "      for cell in row:\n",
        "        if cell == 0:\n",
        "          return False\n",
        "    \n",
        "    # If none of the above conditions are met, the game is a draw\n",
        "    return True\n",
        "  \n",
        "  def get_reward(self):\n",
        "    # Check if player 1 won\n",
        "    if self.check_win(1):\n",
        "      return 1\n",
        "    # Check if player -1 won\n",
        "    elif self.check_win(-1):\n",
        "      return -1\n",
        "    # Otherwise, the game is a draw\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "  def check_win(self, player):\n",
        "    # Check rows\n",
        "    for row in self.board:\n",
        "      if row[0] == row[1] == row[2] == player:\n",
        "        return True\n",
        "    \n",
        "    # Check columns\n",
        "    for col in range(3):\n",
        "      if self.board[0][col] == self.board[1][col] == self.board[2][col] == player:\n",
        "        return True\n",
        "    \n",
        "    # Check diagonals\n",
        "    if self.board[0][0] == self.board[1][1] == self.board[2][2] == player:\n",
        "      return True\n",
        "\n",
        "# Define the AI agent\n",
        "class TicTacToeAgent(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TicTacToeAgent, self).__init__()\n",
        "    self.fc1 = nn.Linear(9, 32, bias=False).t()\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "    self.fc3 = nn.Linear(32, 9)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = torch.FloatTensor(x)\n",
        "      x = torch.relu(self.fc1(x))\n",
        "      x = torch.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# Define the reinforcement learning algorithm\n",
        "def reinforce(env, agent, optimizer, num_episodes):\n",
        "  for episode in range(num_episodes):\n",
        "    state = torch.tensor(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "      action_probs = agent(state)\n",
        "      # Sample an action from the distribution\n",
        "      action = torch.argmax(action_probs)\n",
        "      # Convert from a single integer to a tuple of coordinates\n",
        "      action = (action // 3, action % 3)\n",
        "      next_state, reward, done = env.step(action)\n",
        "      optimizer.zero_grad()\n",
        "      loss = -reward * action_probs[action]\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      state = torch.tensor(next_state)\n",
        "\n",
        "# Train the AI agent\n",
        "env = TicTacToeEnv()\n",
        "agent = TicTacToeAgent()\n",
        "optimizer = optim.Adam(agent.parameters())\n",
        "reinforce(env, agent, optimizer, num_episodes=1000)\n",
        "\n"
      ]
    }
  ]
}